{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/GVHMR-jupyter/blob/main/GVHMR_jupyter.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/facebookresearch/segment-anything-2\n",
        "!wget https://huggingface.co/facebook/sam2-hiera-large/resolve/main/sam2_hiera_large.pt -O /content/sam2_hiera_large.pt\n",
        "!wget https://huggingface.co/facebook/sam2-hiera-large/raw/main/sam2_hiera_l.yaml -O /content/sam2_hiera_l.yaml\n",
        "!wget https://replicate.delivery/pbxt/LSmApgOTlOeLHCvaa6dN4qCJFBK80vUwXPRcf1OAMANF7p8N/source3.mp4 -O /content/input.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "import shutil\n",
        "import subprocess\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "def detect_body_keypoints(frame):\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    img_tensor = F.to_tensor(frame_rgb).unsqueeze(0).to('cuda')\n",
        "    with torch.no_grad():\n",
        "        prediction = body_detector(img_tensor)[0]\n",
        "    if len(prediction['boxes']) > 0:\n",
        "        best_box = prediction['boxes'][prediction['scores'].argmax()].cpu().numpy()\n",
        "        x1, y1, x2, y2 = best_box\n",
        "        center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2\n",
        "        width, height = x2 - x1, y2 - y1\n",
        "        offset_x, offset_y = width * 0.2, height * 0.2\n",
        "        keypoints = np.array([\n",
        "            [center_x, center_y],\n",
        "            [center_x - offset_x, center_y],\n",
        "            [center_x + offset_x, center_y],\n",
        "            [center_x, center_y - offset_y],\n",
        "            [center_x, center_y + offset_y],\n",
        "        ], dtype=np.float32)\n",
        "        keypoints[:, 0] = np.clip(keypoints[:, 0], x1, x2)\n",
        "        keypoints[:, 1] = np.clip(keypoints[:, 1], y1, y2)\n",
        "        return keypoints\n",
        "    else:\n",
        "        height, width = frame.shape[:2]\n",
        "        center = np.array([[width // 2, height // 2]], dtype=np.float32)\n",
        "        return np.tile(center, (5, 1))\n",
        "\n",
        "def remove_background(frame, mask, bg_color):\n",
        "    mask = mask.squeeze()\n",
        "    if mask.dtype == bool:\n",
        "        mask = mask.astype(np.uint8) * 255\n",
        "    else:\n",
        "        mask = (mask > 0).astype(np.uint8) * 255\n",
        "    mask = cv2.resize(mask, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "    bg = np.full(frame.shape, bg_color, dtype=np.uint8)\n",
        "    fg = cv2.bitwise_and(frame, frame, mask=mask)\n",
        "    bg = cv2.bitwise_and(bg, bg, mask=cv2.bitwise_not(mask))\n",
        "    result = cv2.add(fg, bg)\n",
        "    result = clean_hair_area(frame, result, mask, bg_color)\n",
        "    return result\n",
        "\n",
        "def clean_hair_area(original, processed, mask, bg_color):\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    dilated_mask = cv2.dilate(mask, kernel, iterations=2)\n",
        "    hair_edge_mask = cv2.subtract(dilated_mask, mask)\n",
        "    bg_sample = cv2.bitwise_and(original, original, mask=cv2.bitwise_not(dilated_mask))\n",
        "    bg_average = cv2.mean(bg_sample)[:3]\n",
        "    color_distances = np.sqrt(np.sum((original.astype(np.float32) - bg_average) ** 2, axis=2))\n",
        "    color_distances = (color_distances - color_distances.min()) / (color_distances.max() - color_distances.min())\n",
        "    alpha = (1 - color_distances) * (hair_edge_mask / 255.0)\n",
        "    alpha = np.clip(alpha, 0, 1)\n",
        "    for c in range(3):\n",
        "        processed[:, :, c] = processed[:, :, c] * (1 - alpha) + bg_color[c] * alpha\n",
        "    return processed\n",
        "\n",
        "checkpoint = 'sam2_hiera_large.pt'\n",
        "model_cfg = 'sam2_hiera_l.yaml'\n",
        "predictor = build_sam2_video_predictor(model_cfg, checkpoint)\n",
        "body_detector = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "body_detector.eval()\n",
        "body_detector.to(\"cuda\")\n",
        "bg_color='#00FF00'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_video = '/content/input.mp4'\n",
        "bg_color = tuple(int(bg_color.lstrip('#')[i:i + 2], 16) for i in (0, 2, 4))[::-1]\n",
        "frames_dir = \"/content/frames\"\n",
        "if os.path.exists(frames_dir):\n",
        "    shutil.rmtree(frames_dir)\n",
        "os.makedirs(frames_dir, exist_ok=True)\n",
        "ffmpeg_cmd = [\"ffmpeg\", \"-i\", str(input_video), \"-q:v\", \"2\", \"-start_number\", \"0\",f\"{frames_dir}/%05d.jpg\"]\n",
        "result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True, check=True)\n",
        "frame_names = [p for p in os.listdir(frames_dir) if p.endswith(('.jpg', '.jpeg', '.JPG', '.JPEG'))]\n",
        "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
        "inference_state = predictor.init_state(video_path=frames_dir)\n",
        "first_frame_path = os.path.join(frames_dir, frame_names[0])\n",
        "first_frame = cv2.imread(first_frame_path)\n",
        "keypoints = detect_body_keypoints(first_frame)\n",
        "_, out_obj_ids, out_mask_logits = predictor.add_new_points(inference_state=inference_state, frame_idx=0, obj_id=1, points=keypoints, labels=np.ones(len(keypoints), dtype=np.int32))\n",
        "video_segments = {}\n",
        "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
        "    video_segments[out_frame_idx] = {\n",
        "        out_obj_id: out_mask_logits[i].cpu().numpy()\n",
        "        for i, out_obj_id in enumerate(out_obj_ids)\n",
        "    }\n",
        "output_frames_dir = '/content/output_frames'\n",
        "os.makedirs(output_frames_dir, exist_ok=True)\n",
        "frame_count = 0\n",
        "for out_frame_idx in range(len(frame_names)):\n",
        "    frame_path = os.path.join(frames_dir, frame_names[out_frame_idx])\n",
        "    frame = cv2.imread(frame_path)\n",
        "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
        "        frame_with_bg_removed = remove_background(frame, out_mask, bg_color)\n",
        "    output_frame_path = os.path.join(output_frames_dir, f\"{out_frame_idx:05d}.jpg\")\n",
        "    cv2.imwrite(output_frame_path, frame_with_bg_removed)\n",
        "    frame_count += 1\n",
        "output_video_path = '/content/output.mp4'\n",
        "final_video_cmd = [\n",
        "    \"ffmpeg\", \"-y\",\n",
        "    \"-framerate\", \"30\",\n",
        "    \"-i\", f\"{output_frames_dir}/%05d.jpg\",\n",
        "    \"-c:v\", \"libx264\",\n",
        "    \"-pix_fmt\", \"yuv420p\",\n",
        "    output_video_path\n",
        "]\n",
        "result = subprocess.run(final_video_cmd, capture_output=True, text=True, check=True)\n",
        "print(output_video_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
